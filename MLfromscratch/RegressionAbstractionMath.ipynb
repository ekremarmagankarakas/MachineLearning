{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📘 Understanding Regression Models\n",
    "\n",
    "This notebook provides a **mathematical and conceptual understanding** of various regression techniques, including:\n",
    "\n",
    "1. **Linear Regression**\n",
    "2. **Polynomial Regression**\n",
    "3. **Ridge Regression (L2 Regularization)**\n",
    "4. **Lasso Regression (L1 Regularization)**\n",
    "5. **Elastic Net Regression (L1 + L2 Regularization)**\n",
    "\n",
    "Each section explains **the math, when to use the model, and why it's useful**.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 1. Linear Regression\n",
    "\n",
    "Linear Regression models the relationship between **input features ($X$)** and **a continuous target variable ($y$)** using a straight-line equation:\n",
    "\n",
    "$$\n",
    "y = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b\n",
    "$$\n",
    "\n",
    "or in matrix form:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X} \\mathbf{w} + b\n",
    "$$\n",
    "\n",
    "### **Cost Function: Mean Squared Error (MSE)**\n",
    "\n",
    "To measure how well the model fits the data, we minimize the **Mean Squared Error (MSE):**\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $y_i$ = actual value\n",
    "- $\\hat{y}_i$ = predicted value\n",
    "- $m$ = number of samples\n",
    "\n",
    "### **Gradient Descent Optimization**\n",
    "\n",
    "To minimize MSE, we update the weights iteratively using **Gradient Descent**:\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\cdot \\frac{\\partial MSE}{\\partial w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\cdot \\frac{\\partial MSE}{\\partial b}\n",
    "$$\n",
    "\n",
    "Alternatively, we can compute the optimal weights using the **Normal Equation**:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "### **When to Use Linear Regression?**\n",
    "✅ Use when:\n",
    "- Data has a **linear relationship** between features and target.\n",
    "- **No strong multicollinearity** (i.e., features are not highly correlated).\n",
    "- Interpretability is important.\n",
    "\n",
    "❌ Avoid when:\n",
    "- Data is **nonlinear** (use **Polynomial Regression** instead).\n",
    "- There are **many correlated features** (use **Ridge or Lasso**).\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 2. Polynomial Regression\n",
    "\n",
    "Linear Regression assumes a **straight-line relationship**, but **Polynomial Regression** models **curved relationships** by transforming features:\n",
    "\n",
    "For a **degree-$d$** polynomial, we create new features:\n",
    "\n",
    "$$\n",
    "X_{\\text{poly}} = [X, X^2, X^3, ..., X^d]\n",
    "$$\n",
    "\n",
    "Then, we apply **Linear Regression**:\n",
    "\n",
    "$$\n",
    "y = w_1 x + w_2 x^2 + w_3 x^3 + ... + w_d x^d + b\n",
    "$$\n",
    "\n",
    "### **When to Use Polynomial Regression?**\n",
    "✅ Use when:\n",
    "- The data shows **a curved trend**.\n",
    "- You want to model **higher-order interactions**.\n",
    "\n",
    "❌ Avoid when:\n",
    "- **Overfitting** occurs (use **regularization** like **Ridge or Elastic Net**).\n",
    "- High-degree polynomials become unstable.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 3. Ridge Regression (L2 Regularization)\n",
    "\n",
    "Ridge Regression **modifies Linear Regression** by **adding a penalty** on large weights to **prevent overfitting**:\n",
    "\n",
    "### **Ridge Cost Function (MSE + L2 Penalty)**\n",
    "\n",
    "$$\n",
    "MSE + \\alpha \\sum_{j=1}^{n} w_j^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ is a **regularization parameter** (higher values reduce weights more).\n",
    "\n",
    "### **Gradient Descent Update Rule**\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\left( \\frac{\\partial MSE}{\\partial w} + \\frac{\\lambda}{m} w \\right)\n",
    "$$\n",
    "\n",
    "### **When to Use Ridge Regression?**\n",
    "✅ Use when:\n",
    "- Features are **highly correlated (multicollinearity)**.\n",
    "- You need a **balance between performance and interpretability**.\n",
    "\n",
    "❌ Avoid when:\n",
    "- Some features should be **completely eliminated** (use **Lasso** instead).\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 4. Lasso Regression (L1 Regularization)\n",
    "\n",
    "Lasso Regression **improves Ridge** by **shrinking some weights to zero**, which **performs feature selection**.\n",
    "\n",
    "### **Lasso Cost Function (MSE + L1 Penalty)**\n",
    "\n",
    "$$\n",
    "MSE + \\alpha \\sum_{j=1}^{n} |w_j|\n",
    "$$\n",
    "\n",
    "### **Gradient Descent Update Rule**\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\left( \\frac{\\partial MSE}{\\partial w} + \\frac{\\lambda}{m} \\cdot \\text{sign}(w) \\right)\n",
    "$$\n",
    "\n",
    "where **sign($w$)** is:\n",
    "- $+1$ if $w > 0$\n",
    "- $-1$ if $w < 0$\n",
    "- $0$ if $w = 0$\n",
    "\n",
    "### **When to Use Lasso Regression?**\n",
    "✅ Use when:\n",
    "- **Feature selection** is important (some features should have **zero** weight).\n",
    "- You want a **simpler model with fewer variables**.\n",
    "\n",
    "❌ Avoid when:\n",
    "- You need **smooth weight decay** (use **Ridge** instead).\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 5. Elastic Net Regression (L1 + L2 Regularization)\n",
    "\n",
    "Elastic Net **combines Ridge and Lasso**:\n",
    "\n",
    "$$\n",
    "MSE + \\alpha \\left( (1 - r) \\sum w_j^2 + r \\sum |w_j| \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $r$ is the **L1 ratio**:\n",
    "  - If $r = 1$, Elastic Net = **Lasso**.\n",
    "  - If $r = 0$, Elastic Net = **Ridge**.\n",
    "\n",
    "### **When to Use Elastic Net?**\n",
    "✅ Use when:\n",
    "- **Lasso selects too few features**.\n",
    "- **Ridge keeps too many weak features**.\n",
    "- **Multicollinearity exists**, but you also want **sparse features**.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 6. Choosing the Right Regression Model\n",
    "\n",
    "| Model | When to Use |\n",
    "|--------|-----------------------------------------------------|\n",
    "| **Linear Regression** | If the relationship is **linear**. |\n",
    "| **Polynomial Regression** | If the relationship is **nonlinear**. |\n",
    "| **Ridge Regression (L2)** | When **features are correlated**, and you want **smooth weight reduction**. |\n",
    "| **Lasso Regression (L1)** | When you want **automatic feature selection**. |\n",
    "| **Elastic Net Regression** | When you need **both feature selection (L1) and weight decay (L2)**. |\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 7. Final Thoughts\n",
    "\n",
    "- **Linear Regression** is simple but **sensitive to multicollinearity**.\n",
    "- **Polynomial Regression** captures **curved relationships**, but can **overfit**.\n",
    "- **Ridge Regression (L2)** is great for **high-dimensional data** with **multicollinearity**.\n",
    "- **Lasso Regression (L1)** performs **feature selection**, making it useful for **sparse models**.\n",
    "- **Elastic Net** combines **Lasso and Ridge**, balancing both approaches.\n",
    "\n",
    "Choosing the **right model** depends on your **data structure** and **goals**. 🚀\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
