{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Understanding Linear Regression from Scratch\n",
    "\n",
    "## ðŸ“Œ 1. Introduction to Linear Regression\n",
    "\n",
    "Linear Regression predicts a continuous target variable $y$ based on input features $X$. The equation is:\n",
    "\n",
    "$$\n",
    "y = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b\n",
    "$$\n",
    "\n",
    "or in vectorized form:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X} \\mathbf{w} + b\n",
    "$$\n",
    "\n",
    "**Where:**  \n",
    "- $ \\mathbf{X} $ is the input feature matrix.  \n",
    "- $ \\mathbf{w} $ is the weight vector.  \n",
    "- $ b $ is the bias term.  \n",
    "- $ \\mathbf{y} $ is the target variable.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Œ 2. Cost Function (Mean Squared Error)\n",
    "\n",
    "To measure model performance, we use **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "**Where:**  \n",
    "- $ y_i $ is the actual value.  \n",
    "- $ \\hat{y}_i $ is the predicted value.  \n",
    "- $ m $ is the number of samples.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ“Œ 3. Gradient Descent Optimization\n",
    "\n",
    "### **Step 1: Compute Predictions**\n",
    "Predicted values are:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\mathbf{X} \\mathbf{w} + b\n",
    "$$\n",
    "\n",
    "### **Step 2: Compute Gradients**\n",
    "The gradients w.r.t. $w$ and $b$ are:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial w} = \\frac{2}{m} \\mathbf{X}^T (\\mathbf{X} \\mathbf{w} + b - \\mathbf{y})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial b} = \\frac{2}{m} \\sum (\\mathbf{X} \\mathbf{w} + b - \\mathbf{y})\n",
    "$$\n",
    "\n",
    "**Where:**  \n",
    "- $ \\mathbf{X}^T $ is the transpose of $ \\mathbf{X} $.  \n",
    "- $ \\mathbf{X} \\mathbf{w} $ is the predicted output.  \n",
    "\n",
    "### **Step 3: Update Parameters**\n",
    "Using the learning rate $ \\alpha $:\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\cdot \\frac{\\partial MSE}{\\partial w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\cdot \\frac{\\partial MSE}{\\partial b}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Œ 4. Closed-Form Solution (Normal Equation)\n",
    "\n",
    "Instead of iterative optimization, the **Normal Equation** computes optimal weights:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = \\mathbf{w}_0\n",
    "$$\n",
    "\n",
    "**Where:**  \n",
    "- $ (\\mathbf{X}^T \\mathbf{X})^{-1} $ is the inverse of $ \\mathbf{X}^T \\mathbf{X} $.  \n",
    "- $ \\mathbf{X}^T \\mathbf{y} $ is the dot product of $ \\mathbf{X}^T $ and $ \\mathbf{y} $.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Œ 5. Model Evaluation Metrics\n",
    "\n",
    "### **5.1 Mean Squared Error (MSE)**\n",
    "$$\n",
    "MSE = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "### **5.2 RÂ² Score (Coefficient of Determination)**\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum (y - \\hat{y})^2}{\\sum (y - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "**Where:**  \n",
    "- $ \\bar{y} $ is the mean of $ y $.  \n",
    "- $ R^2 $ measures how well the model explains variance in $ y $.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Œ 6. Data Visualization Methods\n",
    "\n",
    "### **6.1 Loss Curve**\n",
    "To visualize **Gradient Descent**, we plot loss over iterations.\n",
    "\n",
    "$$\n",
    "\\text{Loss} = MSE\n",
    "$$\n",
    "\n",
    "### **6.2 Actual vs. Predicted Values**\n",
    "A scatter plot is used to compare **predicted vs actual values**. A perfect model has points on the line $ y = x $.\n",
    "\n",
    "### **6.3 Residuals Plot**\n",
    "Residuals ($ y - \\hat{y} $) should be randomly distributed around zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Œ 7. Conclusion\n",
    "\n",
    "- **Gradient Descent** iteratively optimizes weights.  \n",
    "- **Normal Equation** computes weights directly but is inefficient for large datasets.  \n",
    "- **MSE and RÂ² Score** measure performance.  \n",
    "- **Loss curves and residual plots** help visualize model behavior.  \n",
    "\n",
    "Linear Regression remains a **powerful yet simple** algorithm. ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
