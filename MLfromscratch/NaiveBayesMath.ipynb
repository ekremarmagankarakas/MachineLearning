{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Understanding the Na√Øve Bayes Classifier\n",
    "\n",
    "## üìå 1. Introduction\n",
    "The **Na√Øve Bayes Classifier** is a **probabilistic algorithm** based on **Bayes' Theorem** and assumes that features are **conditionally independent** given the class. It is widely used for **spam detection, sentiment analysis, and medical diagnosis**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå 2. Bayes' Theorem\n",
    "Bayes‚Äô theorem describes the relationship between conditional probabilities:\n",
    "\n",
    "$$\n",
    "P(C|X) = \\frac{P(X|C) P(C)}{P(X)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ P(C|X) $ = **Posterior Probability**: Probability of class $ C $ given data $ X $.\n",
    "- $ P(X|C) $ = **Likelihood**: Probability of data $ X $ given class $ C $.\n",
    "- $ P(C) $ = **Prior Probability**: Probability of class $ C $ occurring.\n",
    "- $ P(X) $ = **Evidence (Normalization Factor)**: Probability of data $ X $ over all classes.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå 3. Applying the Na√Øve Independence Assumption\n",
    "For **multiple features** $ (X_1, X_2, ..., X_n) $, we assume **feature independence**:\n",
    "\n",
    "$$\n",
    "P(X|C) = P(X_1, X_2, ..., X_n | C) = P(X_1 | C) \\cdot P(X_2 | C) \\cdot ... \\cdot P(X_n | C)\n",
    "$$\n",
    "\n",
    "Using this, Bayes' Theorem simplifies to:\n",
    "\n",
    "$$\n",
    "P(C|X) = \\frac{P(C) \\prod_{i=1}^{n} P(X_i | C)}{P(X)}\n",
    "$$\n",
    "\n",
    "Since **$ P(X) $ is constant** across all classes, we only need to compute the **numerator**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå 4. Classification Rule\n",
    "To classify a new data point, we compute **posterior probabilities** for all classes and select the one with the highest probability:\n",
    "\n",
    "$$\n",
    "\\hat{C} = \\arg\\max_{C} P(C) \\prod_{i=1}^{n} P(X_i | C)\n",
    "$$\n",
    "\n",
    "Steps:\n",
    "1. Compute the **prior probability** $ P(C) $.\n",
    "2. Compute the **likelihood** $ P(X_i | C) $ for each feature.\n",
    "3. Multiply them together to get **posterior probabilities**.\n",
    "4. Choose the class with the highest probability.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå 5. Types of Na√Øve Bayes Models\n",
    "\n",
    "### **A. Gaussian Na√Øve Bayes (For Continuous Data)**\n",
    "For **continuous features**, we assume a **Gaussian (Normal) distribution**:\n",
    "\n",
    "$$\n",
    "P(X_i | C) = \\frac{1}{\\sqrt{2 \\pi \\sigma_C^2}} \\exp \\left( -\\frac{(X_i - \\mu_C)^2}{2 \\sigma_C^2} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\mu_C $ = Mean of feature $ X_i $ for class $ C $.\n",
    "- $ \\sigma_C^2 $ = Variance of feature $ X_i $ for class $ C $.\n",
    "\n",
    "---\n",
    "\n",
    "### **B. Multinomial Na√Øve Bayes (For Text Data)**\n",
    "For **discrete data** (e.g., word counts in text classification), we use **Multinomial Na√Øve Bayes**:\n",
    "\n",
    "$$\n",
    "P(X|C) = \\prod_{i=1}^{n} P(X_i | C)^{X_i}\n",
    "$$\n",
    "\n",
    "Using **Laplace Smoothing** (to avoid zero probability):\n",
    "\n",
    "$$\n",
    "P(X_i | C) = \\frac{\\text{count of } X_i \\text{ in } C + 1}{\\sum_{j=1}^{n} \\text{count of } X_j \\text{ in } C + n}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **C. Bernoulli Na√Øve Bayes (For Binary Data)**\n",
    "For **binary features** (e.g., word presence or absence):\n",
    "\n",
    "$$\n",
    "P(X_i | C) = P_i^X (1 - P_i)^{(1 - X)}\n",
    "$$\n",
    "\n",
    "where $ P_i $ is the probability of feature $ X_i $ occurring in class $ C $.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå 6. Using Log Probabilities for Stability\n",
    "Multiplying many small probabilities **causes numerical underflow**, so we take the **logarithm**:\n",
    "\n",
    "$$\n",
    "\\log P(C|X) = \\log P(C) + \\sum_{i=1}^{n} \\log P(X_i | C)\n",
    "$$\n",
    "\n",
    "For classification:\n",
    "\n",
    "$$\n",
    "\\hat{C} = \\arg\\max_{C} \\left( \\log P(C) + \\sum_{i=1}^{n} \\log P(X_i | C) \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üìå 7. Summary of Na√Øve Bayes Models\n",
    "| Model Type | Data Type | Likelihood Assumption |\n",
    "|------------|----------|----------------------|\n",
    "| **Gaussian Na√Øve Bayes** | Continuous (e.g., real numbers) | Normal Distribution |\n",
    "| **Multinomial Na√Øve Bayes** | Categorical (e.g., word counts) | Multinomial Distribution |\n",
    "| **Bernoulli Na√Øve Bayes** | Binary (e.g., word presence) | Bernoulli Distribution |\n",
    "\n",
    "---\n",
    "\n",
    "## üìå 8. When to Use Na√Øve Bayes?\n",
    "‚úÖ **Use Na√Øve Bayes when:**\n",
    "- **Speed is important** (it is very fast).\n",
    "- **Feature independence is reasonable**.\n",
    "- **Text classification** (spam detection, sentiment analysis).\n",
    "- **Medical diagnosis** (predicting diseases from symptoms).\n",
    "\n",
    "‚ùå **Avoid Na√Øve Bayes when:**\n",
    "- **Features are highly correlated**.\n",
    "- **Small datasets with few samples**.\n",
    "- **Complex relationships** (deep learning or ensemble methods work better)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
